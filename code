---
title: "Test"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```
Load and library data 
Example websites:
https://www.r-bloggers.com/2020/05/tidymodels-and-xgbooost-a-few-learnings/
https://www.r-bloggers.com/2020/05/using-xgboost-with-tidymodels/
```{r}
setwd("~/Google Drive/Future_skills/AMA/Data")
ama_dat = read.csv("AMAData.csv", header = TRUE)
library(tidymodels)
library(dplyr)
library(lubridate)
library(naniar)
library(tableone)
library(tidypredict)
```
AMA XGBoost 
##################
Preprocess the data 
##################
Client_ASSESS_Date 
AgeAtAdmission 
DischargeType 
A298_Q7 = Gender
A298_Q6 = Race
```{r}
ama_dat_evaluable = ama_dat %>%
  select(DischargeType, A298_Q7, A298_Q6, AgeAtAdmission, Client_ASSESS_Date)
ama_dat_evaluable = ama_dat_evaluable %>%
  mutate(DischargeType = case_when(
    DischargeType == "AMA Against Medical Advice" ~1, 
    TRUE ~ 0)) %>%
  mutate(female = case_when(
    A298_Q7 == "Female" ~ 1, 
    TRUE ~ 0)) %>%
  mutate(non_white = case_when(
    A298_Q6 == "White or Caucasian" ~ 0,
    TRUE ~ 1)) %>%
  mutate(Client_ASSESS_Date = mdy(Client_ASSESS_Date)) %>%
  mutate(quarter = quarter(Client_ASSESS_Date)) %>%
  select(-c(Client_ASSESS_Date, A298_Q7, A298_Q6)) %>%
  mutate(DischargeType = as.factor(DischargeType)) %>%
  drop_na() 

```
Check descirptives
```{r}
tab1 = CreateTableOne(data = ama_dat_evaluable)
print(tab1, showAllLevels = TRUE)
```


Split the data
```{r}
ama_split = initial_split(ama_dat_evaluable, prop = .30, strata = DischargeType)
ama_training = training(ama_split)
ama_testing = testing(ama_split)
ama_cv_folds =  vfold_cv(data = ama_training, v = 5)
ama_cv_folds
```
Set the engine
Figure out what each of these parameters mean and if there are more

```{r}
xgmodel<-parsnip::boost_tree(
  mode = "classification",
  trees = 1000, #nrounds
  learn_rate = tune(), #eta
  sample_size = tune(), #subsample
  mtry = tune(), #colsample_bytree
  min_n = tune(), #min_child_weight
  tree_depth = tune() #max_depth
) %>%
  set_engine("xgboost", objective = "binary:logistic",
             lambda=0, alpha=1, num_class=3,verbose=1)
```

Need this to help tune the model
What do mtry and finalize mean?
```{r}
xgboostParams <- dials::parameters(
  min_n(),
  tree_depth(),
  learn_rate(),
  finalize(mtry(),select(ama_training,-DischargeType)),
  sample_size = sample_prop(c(0.4, 0.9))
)

set.seed(2020)
### This seems like a randomcv search
xgGrid <- dials::grid_max_entropy(xgboostParams, size = 100)
xgGrid

xgWorkflow <- 
  workflows::workflow() %>%
  add_model(xgmodel) %>% 
  add_formula(DischargeType ~ .)

xgTuned <- tune_grid(
  object = xgWorkflow,
  resamples = ama_cv_folds,
  grid      = xgGrid,
  metrics   = metric_set(bal_accuracy),
  control   = control_grid(verbose = TRUE))
xgTuned
```
Find the best model and select it
Now how do you save the model???
```{r}
xgBestParams = xgTuned %>% 
  tune::select_best(metric = "bal_accuracy")
xgBestParams

xgboost_model_final <- xgmodel %>% 
  finalize_model(xgBestParams)

xgTrainFit<-xgboost_model_final %>% 
  fit(DischargeType ~., data=ama_testing)

xgTrainPreds<- xgTrainFit %>% predict(new_data=ama_testing)
xgTrainPredProbs <- xgTrainFit %>% predict_classprob.model_fit(new_data=ama_testing)
xgTrainPredProbs

parsed_model = parse_model(xgTrainFit)
install.packages("yaml")
library(yaml)
write_yaml(parsed_model, "my_model.yml")

xgTrainFit_read = read_yaml("my_model.yml")
xgTrainFit_par <- as_parsed_model(xgTrainFit_read)
xgTrainPreds <- xgTrainFit_par %>% predict(new_data=ama_testing)
xgTrainPreds =  tidypredict_fit(xgTrainFit_par)

saveRDS(xgTrainFit, "./xgTrainFit.rds")

# later...

# load the model
super_model <- readRDS("./xgTrainFit.rds")
print(super_model)
# make a predictions on "new data" using the final model
test <- super_model %>% predict(new_data=ama_testing)
test

```
