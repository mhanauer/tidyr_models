---
title: "Test"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```
Load and library data 
Example websites:
https://www.r-bloggers.com/2020/05/tidymodels-and-xgbooost-a-few-learnings/
https://www.r-bloggers.com/2020/05/using-xgboost-with-tidymodels/
https://juliasilge.com/blog/xgboost-tune-volleyball/
```{r}
library(tidymodels)
library(dplyr)
library(lubridate)
library(naniar)
library(tableone)
library(DALEX)
library(doParallel)
library(stringr)
library(DALEXtra)
library(mlr)
all_cores <- parallel::detectCores(logical = FALSE)
registerDoParallel(cores = all_cores)
```
Data prep
```{r}
#vb_matches <- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2020/2020-05-19/vb_matches.csv', guess_max = 76000)
vb_matches

vb_parsed <- vb_matches %>%
  transmute(
    circuit,
    gender,
    year,
    w_attacks = w_p1_tot_attacks + w_p2_tot_attacks,
    w_kills = w_p1_tot_kills + w_p2_tot_kills,
    w_errors = w_p1_tot_errors + w_p2_tot_errors,
    w_aces = w_p1_tot_aces + w_p2_tot_aces,
    w_serve_errors = w_p1_tot_serve_errors + w_p2_tot_serve_errors,
    w_blocks = w_p1_tot_blocks + w_p2_tot_blocks,
    w_digs = w_p1_tot_digs + w_p2_tot_digs,
    l_attacks = l_p1_tot_attacks + l_p2_tot_attacks,
    l_kills = l_p1_tot_kills + l_p2_tot_kills,
    l_errors = l_p1_tot_errors + l_p2_tot_errors,
    l_aces = l_p1_tot_aces + l_p2_tot_aces,
    l_serve_errors = l_p1_tot_serve_errors + l_p2_tot_serve_errors,
    l_blocks = l_p1_tot_blocks + l_p2_tot_blocks,
    l_digs = l_p1_tot_digs + l_p2_tot_digs
  ) %>%
  na.omit()

winners <- vb_parsed %>%
  select(circuit, gender, year,
         w_attacks:w_digs) %>%
  rename_with(~ str_remove_all(., "w_"), w_attacks:w_digs) %>%
  mutate(win = "win")

losers <- vb_parsed %>%
  select(circuit, gender, year,
         l_attacks:l_digs) %>%
  rename_with(~ str_remove_all(., "l_"), l_attacks:l_digs) %>%
  mutate(win = "lose")

vb_df <- bind_rows(winners, losers) %>%
  mutate_if(is.character, factor) %>%
  mutate(win = case_when(
    win == "win" ~ 1, 
    TRUE ~ 0)) %>%
  mutate(win = as.factor(win))
vb_df = sample_n(vb_df, 3000)
```
Split the data
```{r}
set.seed(123)
vb_split <- initial_split(vb_df, strata = win)
vb_train <- training(vb_split)
vb_test <- testing(vb_split)

```


Now set up the tunning search specs 
First use a boost_tree with classifcation to set up for xgboost
Then select the hyperparameters (see below)
See: https://parsnip.tidymodels.org/reference/boost_tree.html

Then set the engine, which is the xgboost model and the objective function, which is binary logisitc for a classification problem

For mtry we need to select all possible columns expect for the outcome and need to set that in the hyperparameters selection
For sample size also need to set that
Here for max entropy: https://www.brodrigues.co/blog/2020-03-08-tidymodels/
```{r}
xgb_spec <- boost_tree(
  trees = 1000, 
  tree_depth = tune(), min_n = tune(), 
  loss_reduction = tune(),                     ## first three: model complexity
  sample_size = tune(), mtry = tune(),         ## randomness
  learn_rate = tune(),                         ## step size
) %>% 
  set_engine("xgboost", objective = "binary:logistic") %>% 
  set_mode("classification")
```
Now set up the grid search
```{r}
xgb_grid <- grid_max_entropy(
  tree_depth(),
  min_n(),
  loss_reduction(),
  sample_size = sample_prop(),
  finalize(mtry(), select(vb_train, -win)),
  learn_rate(),
  size = 10
)

```
This is the workflow
It has the formula and then you add the specs
```{r}
xgb_wf <- workflow() %>%
  add_formula(win ~ .) %>%
  add_model(xgb_spec) 
```
Now put together cross validation
Default is 10 cross folds
```{r}
set.seed(123)
vb_folds <- vfold_cv(vb_train, strata = win, v = 5)
```
Now you tune the model
```{r}
doParallel::registerDoParallel()
set.seed(234)
xgb_res <- tune_grid(
  xgb_wf,
  resamples = vb_folds,
  grid = xgb_grid,
  control = control_grid(save_pred = TRUE))
```
Select the best model
Get evaluation metrics
Get predictions
```{r}
best_auc <- select_best(xgb_res, "roc_auc")

final_xgb <- finalize_workflow(
  xgb_wf,
  best_auc
)

final_res <- last_fit(final_xgb, vb_split)

collect_metrics(final_res)
preds =  final_res %>%
  collect_predictions()
preds$.pred_class

collect_metrics(final_res)
```
Save the model
```{r}
saveRDS(final_res, "./final_res.rds")
# later...

# load the model
super_model <- readRDS("./final_res.rds")
```

Need to make the tidymodel work with DALEX
```{r}

fit_dalex <- fit(final_xgb, vb_test)

explainer_2 <- DALEX::explain(fit_dalex, data = select(vb_test, -win), as.numeric(vb_test$win))
explainer_2
vip_gbm <- variable_importance(explainer_2)
vip_gbm
```
Now try shapley values
```{r}
henry = vb_test[1,]

shap_henry <- predict_parts(explainer = explainer_2, 
                      new_observation = henry, 
                                 type = "shap",
                                    B = 25)
plot(shap_henry)
```
Now try the what if analysis
https://ema.drwhy.ai/ceterisParibus.html
```{r}
henry$gender = as.character(henry$gender)
what_if_vb <- predict_profile(explainer = explainer_2, new_observation = henry)
what_if_vb
plot(what_if_vb, variables = c("kills"))

### Women's VB team
plot(what_if_vb, variables = "gender", variable_type = "categorical", categorical_type = "bars")
```

